
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
        <title>Zhizhong Han - Wayne State University </title>
<style type="text/css"></style></head>
    <body><table border="0" width="1024px" align="center"><tbody><tr><td>
    
        </td><td valign="top">
<!--            <img src="images/cmuscslogo.gif">
                <img src="images/rilogo.png"> -->
        <br>
        <table style="font-size: 11pt;" border="0" width="100%">
            <tbody><tr>
                <td>
				<center>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="6"> 
                        <b>Zhizhong Han</b><br><br>
                    </font>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="4"> 
                        Assistant Professor<br>
						Department of Computer Science<br>
                        Wayne State University<br><br>                  
                        Email: h312h at wayne.edu<br>
                        <a href="https://scholar.google.com/citations?user=RGNWczEAAAAJ" border="0">Google Scholar</a>, <a href="https://csrankings.org/#/index?vision&us" border="0">CSRanking</a>, <a href="https://dblp.org/pers/hd/h/Han:Zhizhong" border="0">DBLP</a><br>						
                     <!--   CV: <a href="CV.pdf">PDF</a> <br> -->
                    </font>
				</center>
                </td>
				<td width="40%">
                    <!-- <img width="300" src="./index_files/mypic.jpeg" border="0"> -->
                    <img width="200" src="./Files4Web/h312h.jpg" border="0">
                </td>
            </tr>
        </tbody></table> 
        <p>
        </p><hr size="2" align="left" noshade="">
        <p>
        
		<h2>Bio</h2>
        <font face="helvetica, ariel, &#39;sans serif&#39;"> 
        <!--</font></p><h2><font face="helvetica, ariel, &#39;sans serif&#39;">About Me</font></h2><font face="helvetica, ariel, &#39;sans serif&#39;">-->
		I am an Assistant Professor with the <a href="https://engineering.wayne.edu/cs/">Department of Computer Science</a> at <a href="https://wayne.edu/">Wayne State University</a>. I am the founding director of <a href="https://github.com/MachinePerceptionLab">Machine Perception Lab</a>. Before joining Wayne State University in 01/2021, I spent three wonderful years as a postdoctoral researcher with the <a href="https://www.cs.umd.edu/">Department of Computer Science</a> at the <a href="https://umd.edu/">University of Maryland, College Park</a>, advised by Professor <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>. I obtained my B.E. (2009), M.E. (2012) and Ph.D. (2017) from <a href="http://en.nwpu.edu.cn/">Northwestern Polytechnical University</a> in Pattern Recognition and Intelligent Systems. My research interests include 3D computer vision, digital geometry processing, and artificial intelligence.
        <!-- <br><br> -->
        </p><hr size="2" align="left" noshade="">
		 
		
		<font face="helvetica, ariel, &#50;sans serif&#50;">
		<b><font color="Red">Openings: I am always looking for self-motivated PhD students to work with me. If you are interested in my research, please feel free to send me an email.</font> </b>
		</p><hr size="2" align="left" noshade="">

<div id="news">
<h2>News</h2>
<font face="helvetica, ariel, &#39;sans serif&#39;">
<div style="overflow-y: scroll; height:420px;">
<table>
        <tbody>
		<tr valign="top"> <td><b>[12/2024]</b> </td> <td> Glad to recieve <b><a href="https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/">NVIDIA Academic Grant Program Award</a></b>. Thanks <b><a href="https://www.nvidia.com/en-us/">NVIDIA</a></b> for supporting our research!</td></tr>
		<tr valign="top"> <td><b>[12/2024]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://icml.cc">ICML 2025</a></b>.</td></tr>
		<tr valign="top"> <td><b>[12/2024]</b> </td> <td> Three papers on <b>3D reconstruction</b>, <b>shape modeling</b>, and <b>neural SLAM</b> are accepted at <b><a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a></b>.</td></tr>
		<tr valign="top"> <td><b>[09/2024]</b> </td> <td> Five papers on <b>large vision models</b>, <b>neural implicit representations</b>, <b>neural rendering with 3D Gaussian Splatting</b>, and <b>robust generalization of deep priors</b> are accepted at <b><a href="https://nips.cc/">NeurIPS 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[09/2024]</b> </td> <td> I gave an invited talk on <b>3D Reconstruction through Neural Rendering</b> at <b><a href="https://ai.luddy.indiana.edu/index.html">Luddy AI Center at Indiana University Bloomington</a></b>, hosted by <b><a href="https://luddy.indiana.edu/contact/profile/?David_Crandall">Prof. David Crandall</a></b>.</td></tr>
		<tr valign="top"> <td><b>[09/2024]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2024]</b> </td> <td> One paper on <b>implicit representation learning from silhouette images</b> is accepted at <b><a href="https://2024.acmmm.org/">ACMMM 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2024]</b> </td> <td> Three papers on <b>fast surface reconstruction</b>, <b>open surface reconstruction</b>, and <b>point normal estimation</b> are accepted at <b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2024]</b> </td> <td> Two papers on <b>differentiable volume rendering</b> and <b>unseen shape reconstruction</b> are accepted at <b><a href="https://eccv.ecva.net">ECCV 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2024]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://wacv2025.thecvf.com/">WACV 2025</a></b>.</td></tr>
		<tr valign="top"> <td><b>[05/2024]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://nips.cc/">NeurIPS 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[05/2024]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://bmvc2024.org">BMVC 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[03/2024]</b> </td> <td> One paper on <b>large generative models for UDFs</b> is accepted at <b><a href="https://cvpr.thecvf.com/">CVPR 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[01/2024]</b> </td> <td> One paper on <b>point cloud represenation learning</b> is accepted at <b><a href="https://2024.ieee-icra.org">ICRA 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[12/2023]</b> </td> <td> One paper on <b>point cloud upsampling</b> is accepted at <b><a href="https://aaai.org/aaai-conference/">AAAI 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[09/2023]</b> </td> <td> Three papers on <b>virtual SLAM</b>, <b>image to point cloud registration</b>, and <b>point cloud normal estimation</b> are accepted at <b><a href="https:https://nips.cc/">NeurIPS 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[08/2023]</b> </td> <td> One paper on <b>point cloud normal estimation</b> is accepted at <b><a href="https://asia.siggraph.org/2023/">SIGGRAPH asia 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2023]</b> </td> <td> Four papers on <b>neural implicit representations and point cloud understanding</b> are accepted at <b><a href="https://iccv2023.thecvf.com/">ICCV 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[06/2023]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a></b>.</td></tr>
		<tr valign="top"> <td><b>[05/2023]</b> </td> <td> Glad to receive the <b>Richard Barber Interdisciplinary Research Award</b>.</td></tr>
		<tr valign="top"> <td><b>[04/2023]</b> </td> <td> One paper on <b>Learning neural implicit through mapping noise to noise</b> is accepted for short live presentation at <b><a href="https://icml.cc/">ICML 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[03/2023]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://bmvc2023.org/">BMVC 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[03/2023]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://nips.cc/">NeurIPS 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[02/2023]</b> </td> <td> Five papers on <b>neural implicit representations and point cloud understanding</b> are accepted at <b><a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[12/2022]</b> </td> <td> I gave an invited talk on <b>Neural Implicit Representations for Surface Reconstruction</b> at <b><a href="https://tech.facebook.com/reality-labs/">Meta Reality Labs</a></b>, hosted by <b><a href="https://pages.cs.wisc.edu/~yunyang/">Dr. Yunyang Xiong</a></b>.</td></tr>
		<tr valign="top"> <td><b>[12/2022]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://iccv2023.thecvf.com/">ICCV 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[12/2022]</b> </td> <td> Glad to get invited as a <b>Senior PC member</b> at <b><a href="https://ijcai-23.org/">IJCAI 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[11/2022]</b> </td> <td> I gave an invited talk on <b>Neural Implicit Representations for Surface Reconstruction</b> for <b><a href="https://www.cse.msu.edu/">the Department of CSE at Michigan State University</a></b>, hosted by <b><a href="https://www.cse.msu.edu/~liuxm/index2.html">Prof. Xiaoming Liu</a></b>.</td></tr>
		<tr valign="top"> <td><b>[10/2022]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[10/2022]</b> </td> <td> I gave an invited talk on <b>Neural Implicit Representations for Surface Reconstruction</b> for <b><a href="https://www.cs.cityu.edu.hk/">the Department of CS at City University of Hong Kong</a></b>, hosted by <b><a href="https://sites.google.com/site/junhuihoushomepage/">Prof. Junhui Hou</a></b>.</td></tr>
		<tr valign="top"> <td><b>[09/2022]</b> </td> <td> Two papers on <b>neural implicit representations</b> and <b>point normal estimation</b> are accepted at <b><a href="https://nips.cc/">NeurIPS 2022</a></b>.</td></tr>
		<tr valign="top"> <td><b>[08/2022]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://wacv2023.thecvf.com/">WACV 2023</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2022]</b> </td> <td> Glad to get invited as an <b>Area Chair</b> at <b><a href="https://bmvc2022.org/">BMVC 2022</a></b>.</td></tr>
		<tr valign="top"> <td><b>[07/2022]</b> </td> <td> One paper on <b>neural implicit representations</b> is accepted at <b><a href="https://eccv2022.ecva.net/">ECCV 2022</a></b>.</td></tr>
		<tr valign="top"> <td><b>[03/2022]</b> </td> <td> Four papers on <b>neural implicit representations and point cloud generation</b> are accepted at <b>CVPR</b> 2022.</td></tr>
		<tr valign="top"> <td><b>[03/2022]</b> </td> <td> One paper on <b>point cloud completion</b> is accepted at <b>TPAMI</b>.</td></tr>
		<tr valign="top"> <td><b>[07/2021]</b> </td> <td> Two papers on <b>shape completion and unsupervised structure learning for point clouds</b> are accepted at <b>ICCV</b> 2021.</td></tr>
		<tr valign="top"> <td><b>[06/2021]</b> </td> <td> One paper on <b>unsupervised feature learning for 3D shapes</b> is accepted at <b>ACMMM</b> 2021.</td></tr>
		<tr valign="top"> <td><b>[05/2021]</b> </td> <td> One paper on <b>the learning of implicit function from point clouds</b> is accepted at <b>ICML</b> 2021.</td></tr>
		<tr valign="top"> <td><b>[03/2021]</b> </td> <td> Two papers on <b>3D point clouds completion</b> are accepted at <b>CVPR</b> 2021.</td></tr>
		<tr valign="top"> <td><b>[01/2021]</b> </td> <td> One paper on <b>fine grained 3D shape analysis</b> is accepted at <b>TIP</b>.</td></tr>
		<tr valign="top"> <td><b>[08/2020]</b> </td> <td> Two papers on <b>3D point cloud representation learning</b> and <b>multi-sketch 3D shape modeling</b> are accepted at <b>TIP</b>.</td></tr>
		<tr valign="top"> <td><b>[08/2020]</b> </td> <td> One paper on <b>cross modal representation learning</b> is accepted at <b>TCSVT</b>.</td></tr>
		<tr valign="top"> <td><b>[08/2020]</b> </td> <td> One paper on <b>dense 3D point cloud reconstruction</b> is accepted at <b>WACV</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[08/2020]</b> </td> <td> Two papers on <b>3D shape captioning and 3D point cloud segmentation</b> are accepted at <b>ACMMM</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[07/2020]</b> </td> <td> One paper on <b>supervised learning of 3D implicit function</b> is accepted at <b>ECCV</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[06/2020]</b> </td> <td> One paper on <b>unsupervised structure learning for 3D point clouds</b> is accepted at <b>ICML</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[03/2020]</b> </td> <td> One paper on <b>feature learning for 3D point clouds</b> is accepted at <b>GMP</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[02/2020]</b> </td> <td> Two papers on <b>differentiable rendering for SDF</b> and <b>3D point cloud completion</b> are accepted at <b>CVPR</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[11/2019]</b> </td> <td> Our paper on <b>3D shape completion</b> is accepted at <b>AAAI</b> 2020.</td></tr>
		<tr valign="top"> <td><b>[09/2019]</b> </td> <td> Our paper on <b>small object arrangement in 3D scenes</b> is accepted at <b>TVCG</b>.</td></tr>
		<tr valign="top"> <td><b>[09/2019]</b> </td> <td> Our paper on <b>low rank metric learning</b> is accepted at <b>NeurIPS</b> 2019.</td></tr>
		<tr valign="top"> <td><b>[08/2019]</b> </td> <td> Our paper on <b>3D point cloud understanding from multiple angles</b> is accepted at <b>ICCV</b> 2019.</tr>
		<tr valign="top"> <td><b>[07/2019]</b> </td> <td> One paper on <b>unsupervised 3D point cloud feature learning</b> is accepted at <b>ACMMM</b> 2019.</tr>
		<tr valign="top"> <td><b>[05/2019]</b> </td> <td> Two papers on <b>multi-view based 3D shape understanding</b> are accepted at <b>IJCAI</b> 2019.</tr>
		<tr valign="top"> <td><b>[03/2019]</b> </td> <td> One paper on <b>GAN based 3D scene enhancement generation</b> is accepted at <b>CGI</b> 2019.</tr>
		<tr valign="top"> <td><b>[11/2018]</b> </td> <td> Three papers on <b>unsupervised 3D feature learning</b>, <b>point cloud processing</b>, and <b>3D-Text joint understanding</b> are accepted at <b>AAAI</b> 2019.</tr>
</tbody></table>
</div>   

        </p><hr size="2" align="left" noshade="">
		
		<h2>Academic Service</h2>
		<span style="font-size: 12pt;">
		<table style="font-size: 11pt;" border="0" width="100%">
            <tbody><tr>
                <td width="50%" valign="top">
	    <span style="font-size: 12pt;">
		<b>Conference reviewer: </b><br>
        <span style="font-size: 10pt;">
		NeurIPS: 2021, 2022, 2023 (Area Chair), 2024 (Area Chair)<br>
		ICML: 2022, 2023, 2024, 2025 (Area Chair)<br>
	    ECCV: 2022<br>
		CVPR: 2022, 2023 (Area Chair), 2024 (Area Chair), 2025 (Area Chair)<br>
		ICCV: 2023 (Area Chair)<br>
		SIGGRAPH: 2018, 2022<br>
		SIGGRAPH Asia: 2023<br>
		ICLR: 2022<br>
		AAAI: 2022 (Program Committee)<br>
		IJCAI: 2023 (Senior Program Committee), 2024 (Senior Program Committee)<br>
		WACV: 2022, 2023 (Area Chair), 2025 (Area Chair)<br>
		3DV:  2022<br>
		BMVC: 2022 (Area Chair), 2023 (Area Chair), 2024 (Area Chair)<br>
		<br>
                </td>
				<td>
        <span style="font-size: 12pt;">
        <b>Journal reviewer: </b><br>
        <span style="font-size: 10pt;">
		International Journal of Computer Vision <br>
		IEEE Transactions on Pattern Analysis and Machine Intelligence <br>
		ACM Transactions on Graphics <br>
		IEEE Transactions on Visualization and Computer Graphics<br>
		IEEE Transactions on Image Processing <br>
		IEEE Transactions on Multimedia <br>
		IEEE Transactions on Cybernetics <br>
		IEEE Transactions on Systems, Man and Cybernetics: Systems <br>
        IEEE Transactions on Neural Networks and Learning Systems <br>
		IEEE Transactions on Circuits and Systems for Video Technology <br>
		IEEE Transactions on Medical Imaging <br>
        IEEE Transactions on Industrial Informatics <br>
		IEEE Transactions on Industrial Electronics <br>
		IEEE Journal of Biomedical and Health Informatics<br>
        IEEE Access <br>
		The Visual Computer <br>
		Computer-Aided Design <br>
		Transactions on Machine Learning Research <br>
		ACM Transactions on Multimedia Computing, Communications, and Applications <br>
		ACM Transactions on Asian and Low-Resource Language Information Processing <br>
		EURASIP Journal on Image and Video <br>
		<br>
                </td>
            </tr>
        </tbody></table>  
		
<!--  		
        </p><hr size="2" align="left" noshade="">
          	
		<h2>Academic Service</h2>
		<span style="font-size: 12pt;">
        <b>Conference reviewer: </b><br>
        <span style="font-size: 10pt;">
		SIGGRAPH: 2018, 2022<br>
		NeurIPS: 2021, 2022<br>
		ICML: 2022, 2023<br>
	    ECCV: 2022<br>
		CVPR: 2022, 2023 (Area Chair)<br>
		ICCV: 2023 (Area Chair)<br>
		ICLR: 2022<br>
		AAAI: 2022 (Program Committee)<br>
		IJCAI: 2023 (Senior Program Committee)<br>
		WACV: 2022, 2023 (Area Chair)<br>
		3DV:  2022<br>
		BMVC: 2022 (Area Chair)<br>
		<br><br>
		
        <span style="font-size: 12pt;">
        <b>Journal reviewer: </b><br>
        <span style="font-size: 10pt;">
		International Journal of Computer Vision <br>
		IEEE Transactions on Pattern Analysis and Machine Intelligence <br>
		ACM Transactions on Graphics <br>
		IEEE Transactions on Image Processing <br>
		IEEE Transactions on Multimedia <br>
		IEEE Transactions on Cybernetics <br>
		IEEE Transactions on Systems, Man and Cybernetics: Systems <br>
        IEEE Transactions on Neural Networks and Learning Systems <br>
		IEEE Transactions on Circuits and Systems for Video Technology <br>
		IEEE Transactions on Medical Imaging <br>
        IEEE Transactions on Industrial Informatics <br>
		IEEE Transactions on Industrial Electronics <br>
		IEEE Journal of Biomedical and Health Informatics<br>
        IEEE Access <br>
		The Visual Computer <br>
		Computer-Aided Design <br>
		Transactions on Machine Learning Research <br>
		ACM Transactions on Multimedia Computing, Communications, and Applications <br>
		ACM Transactions on Asian and Low-Resource Language Information Processing <br>
		EURASIP Journal on Image and Video <br>
		<br>
-->	
		
        </p><hr size="2" align="left" noshade="">

        <h2>Publications </h2>
		<span style="font-size: 13pt;">
        <b><font color="white">Preprint</font> </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				
				</tbody>
			</table>
			
			<span style="font-size: 13pt;">
			
		<span style="font-size: 13pt;">
        <b>2024 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/UnseenPointClouds.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes</b><br>
                        <span style="font-size: 10pt;">	
						Chao Chen, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						European Conference on Computer Vision (ECCV), 2024. <br>
                        [<a href="https://www.arxiv.org/pdf/2408.14279">Paper</a>][<a href="https://github.com/chenchao15/Unseen">Code</a>]	
                        <br>
                    </td>
                </tr>
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/VolumeRenderingPrior.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors</b><br>
                        <span style="font-size: 10pt;">	
						<a href="https://wen-yuan-zhang.github.io">Wenyuan Zhang</a>, Kanle Shi, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						European Conference on Computer Vision (ECCV), 2024. <br>
                        [<a href="https://arxiv.org/pdf/2407.16396">Paper</a>][<a href="https://github.com/wen-yuan-zhang/VolumeRenderingPriors">Code</a>]	
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/ImplicitReasoning.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Inferring 3D Occupancy Fields through Implicit Reasoning on Silhouette Images</b><br>
                        <span style="font-size: 10pt;">	
						<a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <B>Zhizhong Han</B><br>
						ACM Multimedia Conference, 2024. <br>
                        [<a href="https://openreview.net/pdf?id=7tvxh1VZK3">Paper</a>]<!--[<a href="https://github.com/junshengzhou/3D-OAE">Code</a>]-->	
                        <br>
                    </td>
                </tr>
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/CVPR2024.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion</b><br>
                        <span style="font-size: 10pt;">	
						<a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, Weiqi Zhang*, <a href="https://mabaorui.github.io/">Baorui Ma</a>, Kanle Shi, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						(* indicates equal contribution)<br>
						IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. <br>
                        [<a href="https://arxiv.org/pdf/2404.06851">Paper</a>][<a href="https://github.com/weiqi-zhang/UDiFF">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/ICRA24OAE.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3D-OAE: Occlusion Auto-Encoders for Self-Supervised Learning on Point Clouds</b><br>
                        <span style="font-size: 10pt;">	
						<a href="https://junshengzhou.github.io/">Junsheng Zhou</a>, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://www.gaoyue.org/">Yue Gao</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <B>Zhizhong Han</B><br>
						IEEE International Conference on Robotics and Automation (ICRA), 2024. (Spotlight)<br>
                        [<a href="https://arxiv.org/pdf/2203.14084.pdf">Paper</a>][<a href="https://github.com/junshengzhou/3D-OAE">Code</a>]
                        <br>
                    </td>
                </tr>
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/AAAI24.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Continuous Implicit Field with Local Distance Indicator for Arbitrary-Scale Point Cloud Upsampling</b><br>
                        <span style="font-size: 10pt;">						
                        Shujuan Li*, <a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						(* indicates equal contribution)<br>
						The AAAI Conference on Artificial Intelligence (AAAI), 2024. <br>
                        [<a href="https://arxiv.org/pdf/2312.15133.pdf">Paper</a>][<a href="https://github.com/lisj575/APU-LDI">Code</a>][<a href="https://lisj575.github.io/APU-LDI">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/CAPUDF_Tpami.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CAP-UDF: Learning Unsigned Distance Functions Progressively from Raw Point Clouds with Consistency-Aware Field Optimization</b> <br>
                        <span style="font-size: 10pt;">	
					    <a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>*, Shujuan Li, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <B>Zhizhong Han</B><br>
                        IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.<br>
						(* indicates equal contribution)<br>
                        [<a href="https://arxiv.org/pdf/2210.02757v3">Paper</a>][<a href="https://github.com/junshengzhou/CAP-UDF">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Noise2NoiseFast.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Fast Learning of Signed Distance Functions from Noisy Point Clouds via Noise to Noise Mapping</b> <br>
                        <span style="font-size: 10pt;">	
					    <a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>*, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
                        IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.<br>
						(* indicates equal contribution)<br>
                        [<a href="https://arxiv.org/pdf/2407.14225v1">Paper</a>][<a href="https://github.com/mabaorui/Noise2NoiseMapping">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Tpami_Normal.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Signed Hyper Surfaces for Oriented Point Cloud Normal Estimation</b> <br>
                        <span style="font-size: 10pt;">	
						Qing Li, Huifang Feng, Kanle Shi, <a href="https://www.gaoyue.org/">Yue Gao</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
                        IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.<br>
                        [<a href="https://arxiv.org/pdf/2305.05873">Paper</a>][<a href="https://github.com/LeoQLi/SHS-Net">Code</a>]
                        <br>
                    </td>
                </tr>
				
				
				</tbody>
			</table>
			
			<span style="font-size: 13pt;">
			
		<span style="font-size: 13pt;">
        <b>2023 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DepthFusionPriors1.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</b><br>
                        <span style="font-size: 10pt;">
						Pengchong Hu, <B>Zhizhong Han</B><br>
						Conference on Neural Information Processing Systems (NeurIPS), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2310.11598.pdf">Paper</a>][<a href="https://github.com/MachinePerceptionLab/Attentive_DFPrior">Code</a>][<a href="https://machineperceptionlab.github.io/Attentive_DF_Prior/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/nips23point2image.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching</b><br>
                        <span style="font-size: 10pt;">
						<a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>*, <a href="https://wen-yuan-zhang.github.io">Wenyuan Zhang</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						Conference on Neural Information Processing Systems (NeurIPS), 2023. (Spotlight)<br>
                        [<a href="https://openreview.net/pdf?id=WjWifKqmcG">Paper</a>][<a href="https://github.com/junshengzhou/VP2P-Match">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/nips23normal.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function</b><br>
                        <span style="font-size: 10pt;">
						Qing Li, Huifang Feng, Kanle Shi, <a href="https://www.gaoyue.org/">Yue Gao</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						Conference on Neural Information Processing Systems (NeurIPS), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2311.00389.pdf">Paper</a>][<a href="https://github.com/LeoQLi/NeuralGF">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/siggraphasianormal.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Neural Gradient Learning and Optimization for Oriented Point Normal Estimation</b><br>
                        <span style="font-size: 10pt;">
						Qing Li, Huifang Feng, Kanle Shi, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						SIGGRAPH Asia, 2023.<br>
                        [<a href="https://arxiv.org/abs/2309.09211">Paper</a>][<a href="https://github.com/LeoQLi/NGLO">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DiscreteCoordinates.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction</b><br>
                        <span style="font-size: 10pt;">
						Sijia Jiang, <a href="https://visiongraphics.github.io">Jing Hua</a>, <B>Zhizhong Han</B><br>
						International Conference on Computer Vision (ICCV), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2308.11025.pdf">Paper</a>][<a href="https://github.com/MachinePerceptionLab/CQ-NIR">Code</a>][<a href="https://machineperceptionlab.github.io/CQ-NIR-page/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/GridPull.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>GridPull: Towards Scalability in Learning Implicit Representations from 3D Point Clouds</b><br>
                        <span style="font-size: 10pt;">
						Chao Chen, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						International Conference on Computer Vision (ICCV), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2308.13175.pdf">Paper</a>][<a href="https://github.com/chenchao15/GridPull">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/iccv23udf.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection</b><br>
                        <span style="font-size: 10pt;">
						<a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>*, Shujuan Li, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						(* indicates equal contribution)<br>
						International Conference on Computer Vision (ICCV), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2308.11441.pdf">Paper</a>][<a href="https://github.com/junshengzhou/LevelSetUDF">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Retro.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation</b> <br>
                        <span style="font-size: 10pt;">					
                        Peng Xiang, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>,  <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.thss.tsinghua.edu.cn/publish/soften/3131/2010/20101219172208224374333/20101219172208224374333_.html">Hui Zhang</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <B>Zhizhong Han</B><br>
                        International Conference on Computer Vision (ICCV), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2308.09314.pdf">Paper</a>][<a href="https://github.com/AllenXiangX/Retro-FPN">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/FewerRays.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Fast Learning Radiance Fields by Shooting Much Fewer Rays</b><br>
                        <span style="font-size: 10pt;">
						<a href="https://wen-yuan-zhang.github.io">Wenyuan Zhang</a>, Ruofan Xing, <a href="https://yunfan.zone/">Yunfan Zeng, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, Kanle Shi, <B>Zhizhong Han</B><br>
						IEEE Transactions on Image Processing, 2023.<br>
                        [<a href="https://arxiv.org/pdf/2208.06821.pdf">Paper</a>][<a href="https://github.com/zParquet/Fast-Learning">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Noise2Noise.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Signed Distance Functions from Noisy 3D Point Clouds via Noise to Noise Mapping</b><br>
                        <span style="font-size: 10pt;">
						<a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						International Conference on Machine Learning (ICML), 2023. (Oral presentation)<br>
                        [<a href="https://arxiv.org/pdf/2306.01405.pdf">Paper</a>][<a href="https://github.com/mabaorui/Noise2NoiseMapping//">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/GradientConsistency.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment</b><br>
                        <span style="font-size: 10pt;">
						<a href="https://mabaorui.github.io/">Baorui Ma</a>*, <a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						(* indicates equal contribution)<br>
						IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2305.11601.pdf">Paper</a>][<a href="https://github.com/mabaorui/TowardsBetterGradient">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/NeuralTPS.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors</b><br>
                        <span style="font-size: 10pt;">
						Chao Chen, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2303.14505.pdf">Paper</a>][<a href="https://github.com/chenchao15/NeuralTPS">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/LPDIF.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>LP-DIF: Learning Local Pattern-specific Deep Implicit Function for 3D Objects and Scenes</b><br>
                        <span style="font-size: 10pt;">
						Meng Wang, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://www.gaoyue.org/">Yue Gao</a>, Kanle Shi, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <B>Zhizhong Han</B><br>
						IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.<br>
                        [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LP-DIF_Learning_Local_Pattern-Specific_Deep_Implicit_Function_for_3D_Objects_CVPR_2023_paper.pdf">Paper</a>][<a href="https://github.com/gtyxyz/lpdif">Code</a>]
                        <br>
                    </td>
                </tr>
				
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/shs-Net.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds</b><br>
                        <span style="font-size: 10pt;">
						Qing Li, Huifang Feng, Kanle Shi, <a href="https://www.gaoyue.org/">Yue Gao</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2305.05873.pdf">Paper</a>][<a href="https://github.com/LeoQLi/SHS-Net">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/parts2words.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching between Parts and Words</b><br>
                        <span style="font-size: 10pt;">
						Chuan Tang, <a href="https://keepthinkingyx.github.io/Xi-Yang/">Xi Yang</a>, <a href="https://bojianwu.github.io/">Bojian Wu</a>, <B>Zhizhong Han</B>, <a href="http://www.yichang-cs.com/">Yi Chang</a><br>
						IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.<br>
                        [<a href="https://arxiv.org/pdf/2107.01872.pdf">Paper</a>][<a href="https://github.com/JLUtangchuan/Parts2Words">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/NeAF.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>NeAF: Learning Neural Angle Fields for Point Normal Estimation</b><br>
                        <span style="font-size: 10pt;">						
                        Shujuan Li*, <a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
						(* indicates equal contribution)<br>
						The AAAI Conference on Artificial Intelligence (AAAI), 2023. <br>
                        [<a href="https://arxiv.org/pdf/2211.16869.pdf">Paper</a>][<a href="https://github.com/lisj575/NeAF/">Code</a>][<a href="https://lisj575.github.io/NeAF/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DNET.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>D-Net: Learning for distinctive point clouds by self-attentive point searching and learnable feature fusion</b><br>
                        <span style="font-size: 10pt;">
						Xinhai Liu, <B>Zhizhong Han</B>, Sanghuk Lee, <a href="https://scholar.google.com/citations?user=50194vkAAAAJ&hl=en">Yan-Pei Cao</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><br>
						Computer Aided Geometric Design, 2023.<br>
                        [<a href="https://arxiv.org/pdf/2305.05842.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				</tbody>
			</table>	
		
		<span style="font-size: 13pt;">
        <b>2022 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/SnowPAMI.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer</b> <br>
                        <span style="font-size: 10pt;">	
						Peng Xiang, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://scholar.google.com/citations?user=50194vkAAAAJ&hl=en">Yan-Pei Cao</a>, Pengfei Wan, Wen Zheng, <B>Zhizhong Han</B><br>	
                        IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.<br>
                        [<a href="https://arxiv.org/pdf/2202.09367.pdf">Paper</a>][<a href="https://github.com/AllenXiangX/SnowflakeNet">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/UDF.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Consistency-Aware Unsigned Distance Functions Progressively from Raw Point Clouds</b> <br>
                        <span style="font-size: 10pt;">	
						<a href="https://junshengzhou.github.io/">Junsheng Zhou</a>*, <a href="https://mabaorui.github.io/">Baorui Ma</a>*, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <B>Zhizhong Han</B><br>
						(* indicates equal contribution)<br>						
                        Conference on Neural Information Processing Systems (NeurIPS), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2210.02757.pdf">Paper</a>][<a href="https://github.com/junshengzhou/CAP-UDF">Code</a>][<a href="https://junshengzhou.github.io/CAP-UDF/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/normal.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>HSurf-Net: Normal Estimation for 3D Point Clouds by Learning Hyper Surfaces</b> <br>
                        <span style="font-size: 10pt;">	
						Qing Li, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.mmrc.iss.ac.cn/~jcheng/">Jin-San Cheng</a>, <a href="http://chwang.xmu.edu.cn/index_en.htm"> Cheng Wang</a>, <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html">Yi Fang</a>, <B>Zhizhong Han</B><br>			
                        Conference on Neural Information Processing Systems (NeurIPS), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2210.07158.pdf">Paper</a>][<a href="https://github.com/LeoQLi/HSurf-Net">Code</a>]<!--[<a href="https://chenchao15.github.io/LPI_page/">Project Page</a>]-->
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/LPI.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Latent Partition Implicit with Surface Codes for 3D Representation</b> <br>
                        <span style="font-size: 10pt;">					
                        Chao Chen, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
                        European Conference on Computer Vision (ECCV), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2207.08631.pdf">Paper</a>][<a href="https://github.com/chenchao15/LPI">Code</a>][<a href="https://chenchao15.github.io/LPI_page/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/SPU-Net.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine Reconstruction with Self-Projection Optimization</b> <br>
                        <span style="font-size: 10pt;">					
                        Xinhai Liu, Xinchen Liu, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
                        IEEE Transactions on Image Processing, 2022.<br>
                        [<a href="https://arxiv.org/pdf/2012.04439.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/LocalNP.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Surface Reconstruction from Point Clouds by Learning Predictive Context Priors</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <B>Zhizhong Han</B><br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2204.11015.pdf">Paper</a>][<a href="https://github.com/mabaorui/PredictableContextPrior">Code</a>][<a href="https://mabaorui.github.io/PredictableContextPrior_page/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/OnSurfacePrior.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Reconstructing Surfaces for Sparse Point Clouds with On-Surface Priors</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <B>Zhizhong Han</B><br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2204.10603.pdf">Paper</a>][<a href="https://github.com/mabaorui/OnSurfacePrior">Code</a>][<a href="https://mabaorui.github.io/-OnSurfacePrior_project_page/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DynamicCodeClouds.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds</b> <br>
                        <span style="font-size: 10pt;">					
                        Tianyang Li, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, Hua Su, <B>Zhizhong Han</B><br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2203.14048.pdf">Paper</a>][<a href="https://github.com/lity20/DCCDIF">Code</a>][<a href="https://lity20.github.io/DCCDIF_project_page/">Project Page</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/AttributesFlow.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <a href="https://junshengzhou.github.io/">Junsheng Zhou, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, Hua Su, <a href="http://jszy.whu.edu.cn/dongzhen/zh_CN/index.htm">Zhen Dong</a>, <B>Zhizhong Han</B><br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.<br>
                        [<a href="https://arxiv.org/pdf/2203.15190.pdf">Paper</a>][<a href="https://github.com/junshengzhou/3DAttriFlow">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/pmp++.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, Peng Xiang, <B>Zhizhong Han</B>, <a href="https://scholar.google.com/citations?user=50194vkAAAAJ&hl=en">Yan-Pei Cao</a>, Pengfei Wan, Wen Zheng, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><br>
                        IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. <br>
                        [<a href="https://arxiv.org/pdf/2202.09507.pdf">Paper</a>]<!--[<a href="https://github.com/diviswen/PMP-Net">Code</a>]-->
                        <br>
                    </td>
                </tr>
				
				</tbody>
			</table>
						
		<span style="font-size: 13pt;">
        <b>2021 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/snow1.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer</b> <br>
                        <span style="font-size: 10pt;">					
                        Peng Xiang, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://scholar.google.com/citations?user=50194vkAAAAJ&hl=en">Yan-Pei Cao</a>, Pengfei Wan, Wen Zheng, <B>Zhizhong Han</B><br>
                        International Conference on Computer Vision (ICCV), 2021. (Oral presentation)<br>
                        [<a href="https://arxiv.org/pdf/2108.04444.pdf">Paper</a>][<a href="https://github.com/AllenXiangX/SnowflakeNet">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/2DMatching1.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projections Matching</b> <br>
                        <span style="font-size: 10pt;">					
                        Chao Chen*, <B>Zhizhong Han</B>*, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						(* indicates equal contribution)<br>
                        International Conference on Computer Vision (ICCV), 2021.<br>
                        [<a href="https://arxiv.org/pdf/2108.03746.pdf">Paper</a>][<a href="https://github.com/chenchao15/2D_projection_matching">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/HVP.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Hierarchical View Predictor: Unsupervised 3D Global Feature Learning through Hierarchical Prediction among Unordered Views</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Xiyang Wang, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        ACM Multimedia conference (ACMMM), 2021. (Oral presentation)<br>
                        [<a href="https://arxiv.org/pdf/2108.03743.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Webp.net-gifmaker.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://mabaorui.github.io/">Baorui Ma</a>*, <B>Zhizhong Han</B>*, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						(* indicates equal contribution)<br>
                        International Conference on Machine Learning (ICML), 2021. (Spotlight)<br>
                        [<a href="https://arxiv.org/pdf/2011.13495.pdf">Paper</a>][<a href="https://github.com/mabaorui/NeuralPull">Code</a>]
                        <br>
                    </td>
                </tr>
				
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/pmp.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, Peng Xiang, <B>Zhizhong Han</B>, <a href="https://scholar.google.com/citations?user=50194vkAAAAJ&hl=en">Yan-Pei Cao</a>, Pengfei Wan, Wen Zheng, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br>
                        [<a href="https://arxiv.org/pdf/2012.03408.pdf">Paper</a>][<a href="https://github.com/diviswen/PMP-Net">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/cyclecompletion.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <B>Zhizhong Han</B>, <a href="https://scholar.google.com/citations?user=50194vkAAAAJ&hl=en">Yan-Pei Cao</a>, Pengfei Wan, Wen Zheng, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br>
                        [<a href="https://arxiv.org/pdf/2103.07838.pdf">Paper</a>][<a href="https://github.com/diviswen/Cycle4Completion">Code</a>]
                        <br>
                    </td>
                </tr>
				
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/FineGrain.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Fine-Grained 3D Shape Classification with Hierarchical Part-View Attentions</b><br>
                        <span style="font-size: 10pt;">						
                        Xinhai Liu, <B>Zhizhong Han</B>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						IEEE Transactions on Image Processing, 2021. <br>
                        [<a href="https://arxiv.org/pdf/2005.12541.pdf">Paper</a>][<a href="https://github.com/liuxinhai/FG3D-Net">Dataset and Code</a>][<a href="https://mp.weixin.qq.com/s/e0YIzM-jcSWqGaD29_0XkA">Best benchmark award in 2021!</a>]
                        <br>
                    </td>
                </tr>				
				</tbody>
			</table>
			
		<span style="font-size: 13pt;">
        <b>2020 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
			
			<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Point2SpatialCapsule.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Point2SpatialCapsule: Aggregating Features and Spatial Relationships of Local Regions on Point Clouds using Spatial-aware Capsules</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <B>Zhizhong Han</B>, Xinhai Liu, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><br>
                        IEEE Transactions on Image Processing, 2020. <br>
                        [<a href="https://arxiv.org/pdf/1908.11026.pdf">Paper</a>][<a href="https://github.com/diviswen/Point2SpatialCapsule">Code</a>]
                        <br>
                    </td>
            </tr>
			
			<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/multisketch.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Reconstructing 3D Shapes from Multiple Sketches using Direct Shape Optimization</b> <br>
                        <span style="font-size: 10pt;">					
                        <B>Zhizhong Han</B>, <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
                        IEEE Transactions on Image Processing, 2020. <br>
                        [<a href="https://ieeexplore.ieee.org/document/9184255">Paper</a>]
                        <br>
                    </td>
            </tr>
			
			<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/wenxintcsv.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CMPD: Using Cross Memory Network with Pair Discrimination for Image-Text Retrieval</b> <br>
                        <span style="font-size: 10pt;">					
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <B>Zhizhong Han</B>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <br>
                        IEEE Transactions on Circuits and Systems for Video Technology, 2020. <br>
                        [<a href="https://ieeexplore.ieee.org/document/9169915">Paper</a>]
                        <br>
                    </td>
                </tr>
			
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DenseReconstruction.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning to Generate Dense Point Clouds with Textures on Multiple Categories</b> <br>
                        <span style="font-size: 10pt;">						
                        <a href="http://www.cs.umd.edu/~taohu/">Tao Hu</a>, Geng Lin, <B>Zhizhong Han</B>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        Winter Conference on Applications of Computer Vision (WACV), 2020. <br>
                        [<a href="https://arxiv.org/pdf/1912.10545.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
			
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/ShapeCaptioner.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>ShapeCaptioner: Generative Caption Network for 3D Shapes by Learning a Mapping from Parts Detected in Multiple Views to Sentences</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Chao Chen, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        ACM Multimedia conference (ACMMM), 2020. (Oral presentation)<br>
                        [<a href="https://arxiv.org/pdf/1908.00120.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/acmmm_cfsis.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CF-SIS: Semantic-Instance Segmentation of 3D Point Clouds by Context Fusion with Self-Attention</b> <br>
                        <span style="font-size: 10pt;">						
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <B>Zhizhong Han</B>, Geunhyuk Youk, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a> <br>
                        ACM Multimedia conference (ACMMM), 2020.<br>
                        [<a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413829">Paper</a>]
                        <br>
                    </td>
                </tr>
			
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/SeqXY2SeqZ.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>SeqXY2SeqZ: Structure Learning for 3D Shapes by Sequentially Predicting 1D Occupancy Segments From 2D Coordinates</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Guanhui Qiao, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        European Conference on Computer Vision (ECCV), 2020. <br>
                        [<a href="https://arxiv.org/pdf/2003.05559.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DRWR.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D Structure Learning from Silhouette Images</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Chao Chen, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        International Conference on Machine Learning (ICML), 2020. <br>
                        [<a href="https://arxiv.org/pdf/2007.06127.pdf">Paper</a>][<a href="https://github.com/chenchao15/drwr">Code</a>]
                        <br>
                    </td>
                </tr>
			
			
			    <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/SDFDiff.png" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization</b> <br>
                        <span style="font-size: 10pt;">						
                        <a href="https://yuejiang-nj.github.io/">Yue Jiang</a>, <a href="https://www.linkedin.com/in/dantong-ji-293b66169/">Dantong Ji</a>, <B>Zhizhong Han</B>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. (Oral presentation)<br>
                        [<a href="https://arxiv.org/pdf/1912.07109.pdf">Paper</a>][<a href="https://github.com/YueJiang-nj/CVPR2020-SDFDiff">Code</a>]
                        <br>
                    </td>
                </tr>
			
			
			    <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Skip-attention.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Point Cloud Completion by Skip-attention Network with Hierarchical Folding</b> <br>
                        <span style="font-size: 10pt;">						
                        Wen Xin, Tianyang Li, <B>Zhizhong Han</B>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a> <br>
                        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>
                        [<a href="https://arxiv.org/pdf/2005.03871.pdf">Paper</a>][<a href="https://github.com/diviswen/sanet">Code</a>]
                        <br>
                    </td>
                </tr>
			
			
			
				<tbody>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/MVC.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
						<b>3D Shape Completion with Multi-view Consistent Inference</b><br>
                        <!--<b>3D Shape Completion with Multi-view Consistent Inference</b> <font color="white">This is some text  </font> <br>-->
                        <span style="font-size: 10pt;">						
                        <a href="http://www.cs.umd.edu/~taohu/">Tao Hu</a>, <B>Zhizhong Han</B>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
                        AAAI Conference on Artificial Intelligence (AAAI), 2020. (Oral presentation) <br>
                        [<a href="https://arxiv.org/pdf/1911.12465.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/LRCNET.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>LRC-Net: Learning Discriminative Features on Point Clouds by Encoding Local Region Contexts</b> <br>
                        <span style="font-size: 10pt;">						
                        Xinhai Liu, <B>Zhizhong Han</B>, Fangzhou Hong, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
                        International Conference on Geometric Modeling and Processing (GMP), 2020. (Oral presentation)<br>
                        [<a href="https://arxiv.org/pdf/2003.08240.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>


				</tbody>
			</table>
			
			
		<span style="font-size: 13pt;">
        <b>2019 </b><br>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>			
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/TVCG_ActiveArrangement.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Active Arrangement of Small Objects in 3D Indoor Scenes</b> <br>
                        <span style="font-size: 10pt;">						
                        Suiyun Zhang, <B>Zhizhong Han</B>, <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <a href="http://www.thss.tsinghua.edu.cn/publish/soften/3131/2010/20101219172208224374333/20101219172208224374333_.html">Hui Zhang</a><br>
						IEEE Transactions on Visualization and Computer Graphics, 2019. <br>
                        [<a href="https://ieeexplore.ieee.org/document/8883084">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/FLRML.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Fast Low-rank Metric Learning for Large-scale and High-dimensional Data</b> <br>
                        <span style="font-size: 10pt;">						
                        Han Liu, <B>Zhizhong Han</B>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.thss.tsinghua.edu.cn/publish/soften/3131/2010/20101219102622467554674/20101219102622467554674_.html">Ming Gu</a><br>
                        Conference on Neural Information Processing Systems (NeurIPS), 2019. <br>
                        [<a href="https://arxiv.org/pdf/1909.06297.pdf">Paper</a>][<a href="https://github.com/highan911/FLRML">Code</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/MAPVAE.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Xiyang Wang, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        International Conference on Computer Vision (ICCV), 2019. <br>
                        [<a href="https://arxiv.org/pdf/1907.12704.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/ShapeCompletion.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Render4Completion: Synthesizing Multi-view Depth Maps for 3D Shape Completion</b> <br>
                        <span style="font-size: 10pt;">						
                        <a href="http://www.cs.umd.edu/~taohu/">Tao Hu</a>, <B>Zhizhong Han</B>, <a href="http://abhinavsh.info/">Abhinav Shrivastava</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
                        International Conference on Computer Vision (ICCV) Workshop on Geometry Meets Deep Learning, 2019. <br>
                        [<a href="https://arxiv.org/pdf/1904.08366.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/L2G.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> L2G Auto-encoder: Understanding Point Clouds by Local-to-Global Reconstruction with Hierarchical Self-Attention</b> <br>
                        <span style="font-size: 10pt;">						
                        Xinhai Liu, <B>Zhizhong Han</B>, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						ACM International Conference on Multimedia (ACMMM), 2019. (Oral presentation) <br>
                        [<a href="https://arxiv.org/pdf/1908.00720.pdf">Paper</a>][<a href="https://github.com/liuxinhai/L2G-AE">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Parts4Feature.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Parts4Feature: Learning 3D Global Features from Generally Semantic Parts in Multiple Views</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Xinhai Liu, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a> <br>
                        International Joint Conference on Artificial Intelligence (IJCAI), 2019. (Oral presentation) <br>
                        [<a href="https://arxiv.org/pdf/1905.07506.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/3DViewGraph.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3DViewGraph: Learning Global Features for 3D Shapes from A Graph of Unordered Views with Attention</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Xiyang Wang, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a> <br>
                        International Joint Conference on Artificial Intelligence (IJCAI), 2019. (Oral presentation) <br>
                        [<a href="https://arxiv.org/pdf/1905.07503.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/3D2SeqViews.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3D2SeqViews: Aggregating Sequential Views for 3D Global Feature Learning by CNN with Hierarchical Attention Aggregation</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Honglei Lu, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a><br>
						IEEE Transactions on Image Processing, 2019. <br>
                        [<a href="https://ieeexplore.ieee.org/document/8666059">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/SeqViews2SeqLabels.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>SeqViews2SeqLabels: Learning 3D Global Features via Aggregating Sequential Views by RNN with Attention</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Mingyang Shang, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a><br>
						IEEE Transactions on Image Processing, 2019. <br>
                        [<a href="https://ieeexplore.ieee.org/document/8453813">Paper</a>][<a href="https://github.com/mingyangShang/SeqViews2SeqLabels">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Y2Seq2Seq.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Y2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by Joint Reconstruction and Prediction of View and Word Sequences</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Mingyang Shang, Xiyang Wang, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						AAAI Conference on Artificial Intelligence (AAAI), 2019. (Oral presentation) <br>
                        [<a href="https://aaai.org/ojs/index.php/AAAI/article/view/3777">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/VIPGAN.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, Mingyang Shang, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						AAAI Conference on Artificial Intelligence (AAAI), 2019. (Spotlight presentation) <br>
                        [<a href="https://arxiv.org/pdf/1811.02744.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Point2Sequence.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network</b> <br>
                        <span style="font-size: 10pt;">						
                        Xinhai Liu, <B>Zhizhong Han</B>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a><br>
						AAAI Conference on Artificial Intelligence (AAAI), 2019. (Oral presentation) <br>
                        [<a href="https://arxiv.org/pdf/1811.02565.pdf">Paper</a>][<a href="https://github.com/liuxinhai/Point2Sequence">Code</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/SSEGAN.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Stylistic Scene Enhancement GAN: Mixed Stylistic Enhancement Generation for 3D Indoor Scenes</b> <br>
                        <span style="font-size: 10pt;">						
                        Suiyun Zhang, <B>Zhizhong Han</B>, <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>, <a href="http://www.cs.umd.edu/~zwicker/index.html">Matthias Zwicker</a>, <a href="http://www.thss.tsinghua.edu.cn/publish/soften/3131/2010/20101219172208224374333/20101219172208224374333_.html">Hui Zhang</a><br>
						Computer Graphics International (CGI), 2019. (Oral presentation) <br>
                        [<a href="https://link.springer.com/article/10.1007/s00371-019-01691-w">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/ICME_Wen.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Adversarial Cross-Modal Retrieval via Learning and Transferring Single-Modal Similarities</b> <br>
                        <span style="font-size: 10pt;">						
                        <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN">Xin Wen</a>, <B>Zhizhong Han</B>, Xinyu Yin, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><br>
						IEEE International Conference on Multimedia and Expo (ICME), 2019. (Oral presentation) <br>
                        [<a href="https://arxiv.org/pdf/1904.08042.pdf">Paper</a>]
                        <br>
                    </td>
                </tr>

				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/ICMR_Emotion.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Emotion Reinforced Visual Storytelling</b> <br>
                        <span style="font-size: 10pt;">						
                        Nanxing Li*, Bei Liu*, <B>Zhizhong Han</B>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://jianlong-fu.github.io/">Jianlong Fu</a><br>
						(* indicates equal contribution)<br>
						ACM International Conference on Multimedia Retrieval (ICMR), 2019. (Oral presentation) <br>
                        [<a href="https://dl.acm.org/citation.cfm?id=3325050&preflayout=tabs">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="120" align="center" src="./Files4Web/BookCCRBM.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Deep Learning for 3D Data Processing</b> <br>
                        <span style="font-size: 10pt;">						
                        <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <B>Zhizhong Han</B>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a><br>
						Deep Learning in Object Detection and Recognition, Springer, Singapore, 2019, pages 155-187.<br>
                        [<a href="https://www.springer.com/gp/book/9789811051517">Book chapter</a>]
                        <br>
                    </td>
                </tr>
				
				</tbody>
			</table>
				
				
		<span style="font-size: 13pt;">
        <b>2018 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/DeepSpatiality.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Deep Spatiality: Unsupervised Learning of Spatially-enhanced Global and Local 3D Features by Deep Neural Network with Coupled Softmax</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a><br>
						IEEE Transactions on Image Processing, 2018. <br>
                        [<a href="https://ieeexplore.ieee.org/document/8318683">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				</tbody>
			</table>
		

		<span style="font-size: 13pt;">
        <b>2017 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Permutation.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Unsupervised Learning of 3D Local Features from Raw Voxels Based on A Novel Permutation voxelization strategy</b> <br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a><br>
						IEEE Transactions on Cybernetics, 2017. <br>
                        [<a href="https://ieeexplore.ieee.org/document/8241450">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/BoSCC.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>BoSCC: Bag of Spatial Context Correlations for Spatially Enhanced 3D Shape Representation</b><br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a><br>
						IEEE Transactions on Image Processing, 2017. <br>
                        [<a href="https://ieeexplore.ieee.org/document/7929304">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/GuideWord.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Semantic 3D Indoor Scene Enhancement Using Guide Words</b> <br>
                        <span style="font-size: 10pt;">						
                        Suiyun Zhang, <B>Zhizhong Han</B>, <a href="https://www.rrm.me.uk/">Ralph R Martin</a>, <a href="http://www.thss.tsinghua.edu.cn/publish/soften/3131/2010/20101219172208224374333/20101219172208224374333_.html">Hui Zhang</a><br>
						Computer Graphics International (CGI), 2017. (Oral presentation) <br>
                        [<a href="https://link.springer.com/article/10.1007/s00371-017-1394-5">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/UserGuided.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>User Guided 3D Scene Enrichment</b> <br>
                        <span style="font-size: 10pt;">						
                        Suiyun Zhang, <B>Zhizhong Han</B>, <a href="http://www.thss.tsinghua.edu.cn/publish/soften/3131/2010/20101219172208224374333/20101219172208224374333_.html">Hui Zhang</a><br>
						 ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry, 2017. (Oral presentation) <br>
                        [<a href="https://dl.acm.org/citation.cfm?id=3014002">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				</tbody>
			</table>
		
		
		<span style="font-size: 13pt;">
        <b>2016 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/CCRBM.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Unsupervised 3D Local Feature Learning by Circle Convolutional Restricted Boltzmann Machine</b><br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a>, <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a><br>
						IEEE Transactions on Image Processing, 2016. <br>
                        [<a href="https://ieeexplore.ieee.org/document/7559748">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/MCRBM.gif" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features with Structure Preservation on 3D Meshes</b><br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="https://www.fst.um.edu.mo/en/staff/fstcmv.html">Chi-Man Vong</a>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a>, <a href="https://www.fst.um.edu.mo/en/staff/pchen.html">C.L.Philip Chen</a><br>
						IEEE Transactions on Neural Networks and Learning Systems, 2016. <br>
                        [<a href="https://ieeexplore.ieee.org/abstract/document/7502161">Paper</a>]
                        <br>
                    </td>
                </tr>
				
				</tbody>
			</table>
		
		
		<span style="font-size: 13pt;">
        <b>2014 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Style.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3D Shape Creation by Style Transfer</b><br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, <a href="http://www.escience.cn/people/JunweiHan/index.html;jsessionid=D41265CB1F75862EA91100C3C9314F9F-n1">Junwei Han</a>, <a href="http://www.adv-ci.com/blog/">Shuhui Bu</a><br>
						The Visual Computer, 2014. <br>
                        [<a href="https://link.springer.com/article/10.1007/s00371-014-0999-1">Paper</a>]
                        <br>
                    </td>
                </tr>				
				</tbody>
			</table>
		
		
		<span style="font-size: 13pt;">
        <b>2011 </b><br>
		<font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
				<tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <center>
                        <img width="300" align="center" src="./Files4Web/Wavelet.jpg" border="0">
                        </center>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>A New Adaptive Wavelet Transform using Lifting Scheme</b><br>
                        <span style="font-size: 10pt;">						
                        <B>Zhizhong Han</B>, <a href="http://www.liuzhenbao.com/">Zhenbao Liu</a>, Yasen Xu<br>
						 International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR), 2011. <br>
                        [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014483">Paper</a>]
                        <br>
                    </td>
                </tr>				
				</tbody>
			</table>

 </p><hr size="2" align="left" noshade="">		
 
<!--
		<h2>Academic Service</h2>
		<span style="font-size: 12pt;">
        <b>Conference reviewer: </b><br>
        <span style="font-size: 10pt;">
		SIGGRAPH: 2018 2022<br>
		NeurIPS: 2021 2022<br>
		ICML: 2022<br>
	    ECCV: 2022<br>
		CVPR: 2022, 2023 (Area Chair)<br>
		ICCV:2023 (Area Chair)<br>
		ICLR: 2022<br>
		AAAI: 2022 (Program Committee)<br>
		IJCAI: 2023 (Senior Program Committee)<br>
		WACV: 2022, 2023 (Area Chair)<br>
		3DV:  2022<br>
		BMVC: 2022 (Area Chair)<br>
		<br><br>
		
        <span style="font-size: 12pt;">
        <b>Journal reviewer: </b><br>
        <span style="font-size: 10pt;">
		International Journal of Computer Vision <br>
		IEEE Transactions on Pattern Analysis and Machine Intelligence <br>
		ACM Transactions on Graphics <br>
		IEEE Transactions on Image Processing <br>
		IEEE Transactions on Multimedia <br>
		IEEE Transactions on Cybernetics <br>
		IEEE Transactions on Systems, Man and Cybernetics: Systems <br>
        IEEE Transactions on Neural Networks and Learning Systems <br>
		IEEE Transactions on Circuits and Systems for Video Technology <br>
		IEEE Transactions on Medical Imaging <br>
        IEEE Transactions on Industrial Informatics <br>
		IEEE Transactions on Industrial Electronics <br>
		IEEE Journal of Biomedical and Health Informatics<br>
        IEEE Access <br>
		The Visual Computer <br>
		Computer-Aided Design <br>
		Transactions on Machine Learning Research <br>
		ACM Transactions on Multimedia Computing, Communications, and Applications <br>
		ACM Transactions on Asian and Low-Resource Language Information Processing <br>
		EURASIP Journal on Image and Video <br>
		<br><br>
		
		<a href="google9af9ffa5a5a05398.html" style="color:white">JustifyByGoogle</a></font>
		
        </p><hr size="2" align="left" noshade="">
 <br><br> -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75897335-1', 'auto');
  ga('send', 'pageview');
</script>


            
</font></td></tr></tbody></table><iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div></body></html>

